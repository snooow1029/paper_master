/**
 * Deep Paper Relationship Analysis Service
 * æ·±åº¦åˆ†æè«–æ–‡Aå¦‚ä½•è«–è¿°è«–æ–‡Bçš„æœå‹™
 * 
 * æ”¹é€²é»ï¼š
 * 1. å¤šå±¤æ¬¡ä¸Šä¸‹æ–‡æå–ï¼ˆæ®µè½ã€ç« ç¯€ã€èªç¾©å¡Šï¼‰
 * 2. çµæ§‹åŒ–è«–æ–‡ä¿¡æ¯æŠ½å–ï¼ˆæ–¹æ³•ã€è²¢ç»ã€å±€é™æ€§ï¼‰
 * 3. è«–è¿°é‚è¼¯æ¨ç†ï¼ˆæ”¯æŒã€åé§ã€æ“´å±•ã€æ¯”è¼ƒï¼‰
 * 4. é—œä¿‚è­‰æ“šå¼·åº¦è©•ä¼°
 */

import axios from 'axios';
import * as cheerio from 'cheerio';
import { PaperMetadata, RelationshipEdge } from './PaperRelationshipAnalyzer';

export interface DeepPaperContext {
  // åŸºæœ¬ä¿¡æ¯
  id: string;
  title: string;
  authors: string[];
  year: string;
  abstract: string;
  venue?: string;
  citationCount?: number; // æ–°å¢ï¼šå¼•ç”¨æ¬¡æ•¸
  
  // çµæ§‹åŒ–å…§å®¹
  structuredContent: {
    introduction: string;
    relatedWork: string;
    methodology: string;
    contributions: string[];
    limitations: string[];
    conclusions: string;
  };
  
  // å¼•ç”¨ä¿¡æ¯
  citationAnalysis: Array<{
    citedPaperId: string;
    citedTitle: string;
    
    // å¤šå±¤æ¬¡ä¸Šä¸‹æ–‡
    sentenceContext: string;      // å¼•ç”¨å¥å­
    paragraphContext: string;     // å¼•ç”¨æ®µè½
    sectionContext: string;       // å¼•ç”¨ç« ç¯€
    
    // èªç¾©ä½ç½®
    section: string;              // æ‰€åœ¨ç« ç¯€
    position: 'early' | 'middle' | 'late';  // ç« ç¯€å…§ä½ç½®
    
    // è«–è¿°é¡å‹
    discourseFunction: 'background' | 'comparison' | 'support' | 'critique' | 'extension' | 'methodology';
    
    // å¼•ç”¨å¯†åº¦
    citationDensity: number;      // è©²æ®µè½å¼•ç”¨å¯†åº¦
    coOccurringCitations: string[]; // åŒæ™‚å‡ºç¾çš„å…¶ä»–å¼•ç”¨
  }>;
}

export interface DeepRelationshipEdge extends RelationshipEdge {
  // è©³ç´°åˆ†æçµæœ
  analysisDetails: {
    // è«–è¿°ç¶­åº¦
    discourseDimensions: {
      methodological: { strength: number; description: string };
      theoretical: { strength: number; description: string };
      empirical: { strength: number; description: string };
      comparative: { strength: number; description: string };
    };
    
    // å¼•ç”¨æ¨¡å¼
    citationPattern: {
      frequency: number;           // å¼•ç”¨é »ç‡
      distribution: string;        // å¼•ç”¨åˆ†å¸ƒï¼ˆé›†ä¸­/åˆ†æ•£ï¼‰
      prominence: number;          // å¼•ç”¨é¡¯è‘—æ€§
      context_diversity: number;   // ä¸Šä¸‹æ–‡å¤šæ¨£æ€§
    };
    
    // èªç¾©é—œä¿‚
    semanticRelation: {
      agreement: number;           // åŒæ„ç¨‹åº¦ (-1 åˆ° 1)
      novelty: number;            // æ–°ç©æ€§è²¢ç» (0 åˆ° 1)
      dependency: number;         // ä¾è³´ç¨‹åº¦ (0 åˆ° 1)
      complementarity: number;    // äº’è£œæ€§ (0 åˆ° 1)
    };
    
    // é—œéµè­‰æ“š
    keyEvidence: Array<{
      text: string;
      section: string;
      importance: number;
      evidenceType: 'direct_quote' | 'paraphrase' | 'comparison' | 'critique' | 'extension';
    }>;
  };
}

export class DeepPaperRelationshipAnalyzer {
  private llmUrl: string;
  private llmModel: string;

  constructor() {
    this.llmUrl = process.env.LOCAL_LLM_URL || 'http://localhost:8000';
    this.llmModel = process.env.LOCAL_LLM_MODEL || 'Qwen/Qwen3-4B-Instruct-2507';
  }

  /**
   * æ¸¬è©¦ LLM é€£æ¥
   */
  async testLLMConnection(): Promise<boolean> {
    try {
      const response = await axios.post(`${this.llmUrl}/v1/chat/completions`, {
        model: this.llmModel,
        messages: [
          {
            role: 'user',
            content: 'Hello, are you working correctly?'
          }
        ],
        max_tokens: 50,
        temperature: 0.3
      }, {
        timeout: 10000
      });

      return response.status === 200 && response.data?.choices?.length > 0;
    } catch (error) {
      console.error('LLM connection test failed:', error);
      return false;
    }
  }

  /**
   * å¾GROBID TEI XMLä¸­æå–æ·±åº¦çµæ§‹åŒ–å…§å®¹
   */
  async extractDeepStructuredContent(teiXml: string, paperMetadata: PaperMetadata): Promise<DeepPaperContext> {
    const $ = cheerio.load(teiXml, { xmlMode: true });
    
    // åŸºæœ¬ä¿¡æ¯æå–
    const paperTitle = $('title[level="a"]').first().text().trim() || paperMetadata.title;
    const paperAuthors = paperMetadata.authors || [];
    
    // çµæ§‹åŒ–å…§å®¹æå–
    const structuredContent = {
      introduction: this.extractSectionContent($, ['introduction', 'intro']),
      relatedWork: this.extractSectionContent($, ['related work', 'background', 'literature review', 'prior work']),
      methodology: this.extractSectionContent($, ['method', 'approach', 'methodology', 'framework', 'model']),
      contributions: await this.extractContributions($),
      limitations: await this.extractLimitations($),
      conclusions: this.extractSectionContent($, ['conclusion', 'conclusions', 'summary'])
    };

    // æ·±åº¦å¼•ç”¨åˆ†æ
    const citationAnalysis = await this.extractDeepCitationAnalysis($, paperMetadata.citations);
    
    console.log(`ğŸ” [DEEP ANALYZER DEBUG] Creating DeepPaperContext:`, {
      citationCount: paperMetadata.citationCount,
      citationCountType: typeof paperMetadata.citationCount,
      title: paperTitle?.substring(0, 50) + '...',
      venue: paperMetadata.venue
    });

    const deepContext = {
      id: paperMetadata.id,
      title: paperTitle,
      authors: paperAuthors,
      year: paperMetadata.year,
      abstract: paperMetadata.abstract || '',
      venue: paperMetadata.venue,
      citationCount: paperMetadata.citationCount, // æ–°å¢ï¼šå¼•ç”¨æ¬¡æ•¸
      structuredContent,
      citationAnalysis
    };
    
    console.log(`ğŸ” [DEEP ANALYZER DEBUG] Final DeepPaperContext citationCount:`, {
      citationCount: deepContext.citationCount,
      citationCountType: typeof deepContext.citationCount
    });
    
    return deepContext;
  }

  /**
   * æå–ç‰¹å®šç« ç¯€å…§å®¹
   */
  private extractSectionContent($: cheerio.CheerioAPI, sectionKeywords: string[]): string {
    let content = '';
    
    // å˜—è©¦å¤šç¨®ç« ç¯€åŒ¹é…ç­–ç•¥
    for (const keyword of sectionKeywords) {
      // ç²¾ç¢ºåŒ¹é…
      const exactMatch = $(`div[type="section"] head`).filter((_, el) => {
        return $(el).text().trim().toLowerCase() === keyword.toLowerCase();
      });
      
      if (exactMatch.length > 0) {
        const section = exactMatch.first().parent();
        content = section.clone().children('head').remove().end().text().trim();
        if (content.length > 100) break; // æ‰¾åˆ°æœ‰æ•ˆå…§å®¹å°±åœæ­¢
      }
      
      // æ¨¡ç³ŠåŒ¹é…
      if (!content) {
        const fuzzyMatch = $(`div[type="section"] head`).filter((_, el) => {
          return $(el).text().trim().toLowerCase().includes(keyword.toLowerCase());
        });
        
        if (fuzzyMatch.length > 0) {
          const section = fuzzyMatch.first().parent();
          content = section.clone().children('head').remove().end().text().trim();
          if (content.length > 100) break;
        }
      }
    }
    
    return content;
  }

  /**
   * ä½¿ç”¨LLMæå–è«–æ–‡è²¢ç»
   */
  private async extractContributions($: cheerio.CheerioAPI): Promise<string[]> {
    // å¾å¤šå€‹å¯èƒ½çš„ç« ç¯€æå–è²¢ç»ç›¸é—œå…§å®¹
    const candidateSections = [
      this.extractSectionContent($, ['contribution', 'contributions']),
      this.extractSectionContent($, ['introduction', 'intro']),
      this.extractSectionContent($, ['abstract']),
      this.extractSectionContent($, ['conclusion', 'conclusions'])
    ].filter(text => text.length > 50);

    if (candidateSections.length === 0) return [];

    try {
      const prompt = `
Extract the main contributions of this research paper from the following sections:

${candidateSections.map((text, i) => `**Section ${i + 1}:**\n${text.substring(0, 1000)}`).join('\n\n')}

Please list 3-5 main contributions in the following JSON format:
{
  "contributions": [
    "First main contribution...",
    "Second main contribution...",
    ...
  ]
}

Focus on novel methods, theoretical insights, empirical findings, or practical applications.`;

      const response = await this.callLLMWithRetry([
        {
          role: 'system',
          content: 'You are an expert academic researcher. Extract key contributions from research papers accurately and concisely.'
        },
        {
          role: 'user',
          content: prompt
        }
      ]);

      const result = this.parseJsonResponse(response.data.choices[0].message.content);
      return result?.contributions || [];
      
    } catch (error) {
      console.error('Failed to extract contributions:', error);
      return [];
    }
  }

  /**
   * ä½¿ç”¨LLMæå–è«–æ–‡å±€é™æ€§
   */
  private async extractLimitations($: cheerio.CheerioAPI): Promise<string[]> {
    const candidateSections = [
      this.extractSectionContent($, ['limitation', 'limitations']),
      this.extractSectionContent($, ['discussion']),
      this.extractSectionContent($, ['conclusion', 'conclusions']),
      this.extractSectionContent($, ['future work', 'future research'])
    ].filter(text => text.length > 50);

    if (candidateSections.length === 0) return [];

    try {
      const prompt = `
Identify the limitations, challenges, or areas for improvement mentioned in this research paper:

${candidateSections.map((text, i) => `**Section ${i + 1}:**\n${text.substring(0, 1000)}`).join('\n\n')}

Please list the main limitations in the following JSON format:
{
  "limitations": [
    "First limitation...",
    "Second limitation...",
    ...
  ]
}

Focus on methodological limitations, dataset constraints, scope restrictions, or acknowledged weaknesses.`;

      const response = await this.callLLMWithRetry([
        {
          role: 'system',
          content: 'You are an expert academic researcher. Identify research limitations accurately and objectively.'
        },
        {
          role: 'user',
          content: prompt
        }
      ]);

      const result = this.parseJsonResponse(response.data.choices[0].message.content);
      return result?.limitations || [];
      
    } catch (error) {
      console.error('Failed to extract limitations:', error);
      return [];
    }
  }

  /**
   * æ·±åº¦å¼•ç”¨åˆ†æ
   */
  private async extractDeepCitationAnalysis(
    $: cheerio.CheerioAPI,
    citations: PaperMetadata['citations']
  ): Promise<DeepPaperContext['citationAnalysis']> {
    const analysisResults: DeepPaperContext['citationAnalysis'] = [];

    for (const citation of citations) {
      // æ‰¾åˆ°å¼•ç”¨ä½ç½®
      const citationRefs = $(`ref[target="#${citation.id}"]`);
      
      for (let i = 0; i < citationRefs.length; i++) {
        const $ref = citationRefs.eq(i);
        
        // å¤šå±¤æ¬¡ä¸Šä¸‹æ–‡æå–
        const sentenceContext = this.extractSentenceContext($ref);
        const paragraphContext = this.extractParagraphContext($ref);
        const sectionContext = this.extractSectionContext($ref);
        
        // èªç¾©ä½ç½®åˆ†æ
        const section = this.getContainingSection($ref);
        const position = this.analyzePositionInSection($ref);
        
        // è«–è¿°åŠŸèƒ½åˆ†æ
        const discourseFunction = await this.analyzeDiscourseFunction(
          sentenceContext,
          paragraphContext,
          citation.title || ''
        );
        
        // å¼•ç”¨å¯†åº¦è¨ˆç®—
        const citationDensity = this.calculateCitationDensity($ref);
        const coOccurringCitations = this.findCoOccurringCitations($ref);

        analysisResults.push({
          citedPaperId: citation.id,
          citedTitle: citation.title || 'Unknown',
          sentenceContext,
          paragraphContext,
          sectionContext,
          section,
          position,
          discourseFunction,
          citationDensity,
          coOccurringCitations
        });
      }
    }

    return analysisResults;
  }

  /**
   * æå–å¥å­ç´šä¸Šä¸‹æ–‡
   */
  private extractSentenceContext($ref: cheerio.Cheerio<any>): string {
    const sentence = $ref.closest('s');
    if (sentence.length > 0) {
      return sentence.text().trim();
    }
    
    // å‚™ç”¨ï¼šæ‰¾åˆ°åŒ…å«å¼•ç”¨çš„å®Œæ•´å¥å­
    const paragraph = $ref.closest('p');
    const fullText = paragraph.text();
    const refText = $ref.text();
    const refIndex = fullText.indexOf(refText);
    
    if (refIndex !== -1) {
      // æ‰¾åˆ°å¥å­é‚Šç•Œ
      const beforeText = fullText.substring(0, refIndex);
      const afterText = fullText.substring(refIndex + refText.length);
      
      const sentenceStart = Math.max(
        beforeText.lastIndexOf('. ') + 2,
        beforeText.lastIndexOf('! ') + 2,
        beforeText.lastIndexOf('? ') + 2,
        0
      );
      
      const sentenceEnd = Math.min(
        afterText.indexOf('. ') !== -1 ? refIndex + refText.length + afterText.indexOf('. ') + 1 : fullText.length,
        afterText.indexOf('! ') !== -1 ? refIndex + refText.length + afterText.indexOf('! ') + 1 : fullText.length,
        afterText.indexOf('? ') !== -1 ? refIndex + refText.length + afterText.indexOf('? ') + 1 : fullText.length
      );
      
      return fullText.substring(sentenceStart, sentenceEnd).trim();
    }
    
    return $ref.text();
  }

  /**
   * æå–æ®µè½ç´šä¸Šä¸‹æ–‡
   */
  private extractParagraphContext($ref: cheerio.Cheerio<any>): string {
    const paragraph = $ref.closest('p');
    return paragraph.text().trim();
  }

  /**
   * æå–ç« ç¯€ç´šä¸Šä¸‹æ–‡
   */
  private extractSectionContext($ref: cheerio.Cheerio<any>): string {
    const section = $ref.closest('div[type="section"]');
    return section.text().trim().substring(0, 2000); // é™åˆ¶é•·åº¦
  }

  /**
   * ç²å–åŒ…å«ç« ç¯€åç¨±
   */
  private getContainingSection($ref: cheerio.Cheerio<any>): string {
    const section = $ref.closest('div[type="section"]');
    const head = section.find('head').first();
    return head.text().trim() || 'Unknown Section';
  }

  /**
   * åˆ†æåœ¨ç« ç¯€ä¸­çš„ä½ç½®
   */
  private analyzePositionInSection($ref: cheerio.Cheerio<any>): 'early' | 'middle' | 'late' {
    const section = $ref.closest('div[type="section"]');
    const allParagraphs = section.find('p');
    const currentParagraph = $ref.closest('p');
    
    const paragraphIndex = allParagraphs.index(currentParagraph);
    const totalParagraphs = allParagraphs.length;
    
    if (paragraphIndex < totalParagraphs * 0.3) return 'early';
    if (paragraphIndex > totalParagraphs * 0.7) return 'late';
    return 'middle';
  }

  /**
   * ä½¿ç”¨LLMåˆ†æè«–è¿°åŠŸèƒ½
   */
  private async analyzeDiscourseFunction(
    sentence: string,
    paragraph: string,
    citedTitle: string
  ): Promise<DeepPaperContext['citationAnalysis'][0]['discourseFunction']> {
    try {
      const prompt = `
Analyze the discourse function of this citation in academic writing:

**Cited Paper**: ${citedTitle}

**Citation Sentence**: ${sentence}

**Paragraph Context**: ${paragraph.substring(0, 500)}

Determine the primary discourse function from these categories:
- background: Providing background/context information
- comparison: Comparing with current work
- support: Supporting current claims/arguments
- critique: Criticizing or identifying limitations
- extension: Extending or building upon the cited work
- methodology: Referencing methods or techniques

Respond with only the category name (e.g., "support", "comparison", etc.).`;

      const response = await this.callLLMWithRetry([
        {
          role: 'system',
          content: 'You are an expert in academic discourse analysis. Classify citation functions accurately based on context.'
        },
        {
          role: 'user',
          content: prompt
        }
      ]);

      const result = response.data.choices[0].message.content.trim().toLowerCase();
      const validFunctions: DeepPaperContext['citationAnalysis'][0]['discourseFunction'][] = 
        ['background', 'comparison', 'support', 'critique', 'extension', 'methodology'];
      
      return validFunctions.find(fn => result.includes(fn)) || 'background';
      
    } catch (error) {
      console.error('Failed to analyze discourse function:', error);
      return 'background';
    }
  }

  /**
   * è¨ˆç®—å¼•ç”¨å¯†åº¦
   */
  private calculateCitationDensity($ref: cheerio.Cheerio<any>): number {
    const paragraph = $ref.closest('p');
    const citations = paragraph.find('ref[type="bibr"]');
    const words = paragraph.text().split(/\s+/).length;
    
    return citations.length / Math.max(words, 1) * 100; // æ¯100è©çš„å¼•ç”¨æ•¸
  }

  /**
   * æ‰¾åˆ°åŒæ™‚å‡ºç¾çš„å…¶ä»–å¼•ç”¨
   */
  private findCoOccurringCitations($ref: cheerio.Cheerio<any>): string[] {
    const paragraph = $ref.closest('p');
    const otherCitations = paragraph.find('ref[type="bibr"]');
    const currentTarget = $ref.attr('target');
    
    const coOccurring: string[] = [];
    otherCitations.each((_, el) => {
      const cheerioEl = paragraph.find(el);
      const target = cheerioEl.attr('target');
      if (target && target !== currentTarget) {
        coOccurring.push(target.substring(1)); // ç§»é™¤ # ç¬¦è™Ÿ
      }
    });
    
    return [...new Set(coOccurring)]; // å»é‡
  }

  /**
   * æ·±åº¦é—œä¿‚åˆ†æ
   */
  async analyzeDeepRelationship(
    sourcePaper: DeepPaperContext,
    targetPaper: DeepPaperContext
  ): Promise<DeepRelationshipEdge | null> {
    // æ‰¾åˆ°ç›¸é—œçš„å¼•ç”¨åˆ†æ
    const relevantCitations = sourcePaper.citationAnalysis.filter(
      citation => citation.citedPaperId === targetPaper.id || 
                  this.isSimilarTitle(citation.citedTitle, targetPaper.title)
    );

    if (relevantCitations.length === 0) {
      return null;
    }

    try {
      // æ§‹å»ºæ·±åº¦åˆ†ææç¤º
      const analysisPrompt = this.buildDeepAnalysisPrompt(
        sourcePaper, 
        targetPaper, 
        relevantCitations
      );

      const response = await this.callLLMWithRetry([
        {
          role: 'system',
          content: `You are an expert academic analyst specializing in scholarly discourse and citation analysis. 
          Your task is to perform deep relationship analysis between academic papers, considering methodological, 
          theoretical, empirical, and comparative dimensions.`
        },
        {
          role: 'user',
          content: analysisPrompt
        }
      ]);

      const analysis = response.data.choices[0].message.content;
      return this.parseDeepAnalysisResult(analysis, sourcePaper, targetPaper, relevantCitations);

    } catch (error) {
      console.error('Deep relationship analysis failed:', error);
      return null;
    }
  }

  /**
   * æ§‹å»ºæ·±åº¦åˆ†ææç¤º
   */
  private buildDeepAnalysisPrompt(
    sourcePaper: DeepPaperContext,
    targetPaper: DeepPaperContext,
    citations: DeepPaperContext['citationAnalysis']
  ): string {
    return `
Perform deep relationship analysis between two academic papers:

**Source Paper**: "${sourcePaper.title}" (${sourcePaper.year})
- Authors: ${sourcePaper.authors.join(', ')}
- Abstract: ${sourcePaper.abstract.substring(0, 300)}...
- Main Contributions: ${sourcePaper.structuredContent.contributions.join('; ')}
- Limitations: ${sourcePaper.structuredContent.limitations.join('; ')}

**Target Paper**: "${targetPaper.title}" (${targetPaper.year})
- Authors: ${targetPaper.authors.join(', ')}
- Abstract: ${targetPaper.abstract.substring(0, 300)}...

**Citation Analysis**:
${citations.map((c, i) => `
Citation ${i + 1}:
- Section: ${c.section}
- Position: ${c.position}
- Function: ${c.discourseFunction}
- Sentence: "${c.sentenceContext}"
- Paragraph Context: "${c.paragraphContext.substring(0, 200)}..."
- Co-occurring Citations: ${c.coOccurringCitations.join(', ') || 'None'}
`).join('\n')}

**Analysis Required**:
Provide a comprehensive JSON analysis with the following structure:

{
  "relationship_type": "builds_on|extends|applies|compares|surveys|critiques",
  "strength": 0.0-1.0,
  "evidence": "Brief description of key evidence",
  "description": "Detailed relationship description",
  
  "discourse_dimensions": {
    "methodological": {"strength": 0.0-1.0, "description": "How methodologically related"},
    "theoretical": {"strength": 0.0-1.0, "description": "How theoretically related"},
    "empirical": {"strength": 0.0-1.0, "description": "How empirically related"},
    "comparative": {"strength": 0.0-1.0, "description": "How comparative analysis is done"}
  },
  
  "citation_pattern": {
    "frequency": ${citations.length},
    "distribution": "concentrated|distributed",
    "prominence": 0.0-1.0,
    "context_diversity": 0.0-1.0
  },
  
  "semantic_relation": {
    "agreement": -1.0-1.0,
    "novelty": 0.0-1.0,
    "dependency": 0.0-1.0,
    "complementarity": 0.0-1.0
  },
  
  "key_evidence": [
    {
      "text": "Important quote or paraphrase",
      "section": "Section name",
      "importance": 0.0-1.0,
      "evidence_type": "direct_quote|paraphrase|comparison|critique|extension"
    }
  ]
}

Focus on providing accurate numerical assessments and detailed qualitative descriptions.`;
  }

  /**
   * è§£ææ·±åº¦åˆ†æçµæœ
   */
  private parseDeepAnalysisResult(
    analysis: string,
    sourcePaper: DeepPaperContext,
    targetPaper: DeepPaperContext,
    citations: DeepPaperContext['citationAnalysis']
  ): DeepRelationshipEdge | null {
    try {
      const result = this.parseJsonResponse(analysis);
      if (!result) return null;

      const basicEdge: RelationshipEdge = {
        source: sourcePaper.id,
        target: targetPaper.id,
        relationship: result.relationship_type || 'builds_on',
        strength: result.strength || 0.5,
        evidence: result.evidence || 'Citation analysis',
        description: result.description || `${sourcePaper.title} cites ${targetPaper.title}`
      };

      const deepEdge: DeepRelationshipEdge = {
        ...basicEdge,
        analysisDetails: {
          discourseDimensions: result.discourse_dimensions || {
            methodological: { strength: 0.5, description: 'Unknown' },
            theoretical: { strength: 0.5, description: 'Unknown' },
            empirical: { strength: 0.5, description: 'Unknown' },
            comparative: { strength: 0.5, description: 'Unknown' }
          },
          citationPattern: result.citation_pattern || {
            frequency: citations.length,
            distribution: 'distributed',
            prominence: 0.5,
            context_diversity: 0.5
          },
          semanticRelation: result.semantic_relation || {
            agreement: 0.5,
            novelty: 0.5,
            dependency: 0.5,
            complementarity: 0.5
          },
          keyEvidence: result.key_evidence || []
        }
      };

      return deepEdge;

    } catch (error) {
      console.error('Failed to parse deep analysis result:', error);
      return null;
    }
  }

  // è¼”åŠ©æ–¹æ³•
  private async callLLMWithRetry(
    messages: Array<{role: string, content: string}>,
    maxRetries: number = 2
  ): Promise<any> {
    for (let attempt = 1; attempt <= maxRetries + 1; attempt++) {
      try {
        const response = await axios.post(`${this.llmUrl}/v1/chat/completions`, {
          model: this.llmModel,
          messages,
          max_tokens: 2000,
          temperature: 0.1 // è¼ƒä½æº«åº¦ç¢ºä¿ä¸€è‡´æ€§
        }, {
          timeout: 60000 // å¢åŠ è¶…æ™‚æ™‚é–“
        });

        if (response.status === 200 && response.data?.choices?.length > 0) {
          return response;
        }
        
        throw new Error(`LLM returned invalid response: ${response.status}`);
      } catch (error) {
        console.warn(`LLM call attempt ${attempt}/${maxRetries + 1} failed:`, error instanceof Error ? error.message : error);
        
        if (attempt <= maxRetries) {
          const delay = Math.min(2000 * Math.pow(2, attempt - 1), 10000);
          console.log(`Retrying in ${delay}ms...`);
          await this.sleep(delay);
        } else {
          throw error;
        }
      }
    }
  }

  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  private parseJsonResponse(text: string): any {
    try {
      // å˜—è©¦æå–JSONéƒ¨åˆ†
      const jsonMatch = text.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0]);
      }
      return JSON.parse(text);
    } catch (error) {
      console.error('Failed to parse JSON response:', error);
      return null;
    }
  }

  private isSimilarTitle(title1: string, title2: string): boolean {
    const normalize = (str: string) => str.toLowerCase()
      .replace(/[^\w\s]/g, '')
      .replace(/\s+/g, ' ')
      .trim();
    
    const normalized1 = normalize(title1);
    const normalized2 = normalize(title2);
    
    // ç°¡å–®çš„ç›¸ä¼¼åº¦æª¢æŸ¥
    const words1 = normalized1.split(' ');
    const words2 = normalized2.split(' ');
    
    const intersection = words1.filter(word => 
      word.length > 3 && words2.includes(word)
    );
    
    return intersection.length >= Math.min(words1.length, words2.length) * 0.5;
  }
}
